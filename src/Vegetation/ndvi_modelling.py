# -*- coding: utf-8 -*-
"""ndvi_modelling3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SG9dj0wkjRkisAooHdc3lJ9ggSX-1MQH
"""

from google.colab import drive
drive.mount('/content/gdrive')

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor

os.chdir('/content/gdrive/MyDrive/Agri_Policy_Proj')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#device = torch.device("cpu")

!pip install joblib

!pip install einops

pickle_path = '/content/gdrive/MyDrive/Agri_Policy_Proj/ndvi_yield_all.pkl'
parent_directory = '/content/gdrive/MyDrive/Agri_Policy_Proj'

def min_max_normalization(input_array):
    min = input_array.min()
    max = input_array.max()
    return (input_array - min) / (max - min)

def load_ndvi_yield_data(pickle_path):
    try:
        # Load the pickled dictionary
        with open(pickle_path, 'rb') as f:
            ndvi_yield_preprocessed = pickle.load(f)
            ndvi = ndvi_yield_preprocessed["NDVI_Array"]
            yields = ndvi_yield_preprocessed["Yield"]
            year = ndvi_yield_preprocessed["Year"]
            county = ndvi_yield_preprocessed["County_Name"]
            ndvi_new = np.expand_dims(ndvi, axis=1)
            ndvi_normalized = min_max_normalization(ndvi_new)
        return ndvi_yield_preprocessed, ndvi_normalized, yields, year, county
    except FileNotFoundError:
        print(f"Error: File '{pickle_path}' not found.")
        return None
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

ndvi_yield_preprocessed, ndvi_normalized, yields, year, county = load_ndvi_yield_data(pickle_path)

# Define a custom dataset
class CustomDataset(Dataset):
  def __init__(self, data):
      self.data = data

  def __len__(self):
      return len(self.data)

  def __getitem__(self, idx):
      return self.data[idx]

"""### RESNET50 + PCA + Random Forrest"""

def resnet50_features(input_array, batch_size):
    # Instantiate ResNet-50 model
    resnet = models.resnet50(weights=None)  # Load without pretrained weights
    resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
    resnet.fc = nn.Identity()  # Remove the final fully connected layer

    # Assuming you have defined 'device' somewhere in your code
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    resnet.to(device)

    num_batches = len(input_array) // batch_size

    # Define the dataset and data loader
    dataset = CustomDataset(input_array)
    data_loader = DataLoader(dataset, batch_size=batch_size)

    # List to store concatenated feature vectors from all batches
    all_features = []

    # Set the model to evaluation mode
    resnet.eval()

    # Iterate over the data loader
    for batch_data in data_loader:
        # Pass the batch through ResNet-50
        with torch.no_grad():
            batch_features = resnet(batch_data.to(device, dtype=torch.float32))

        # Append the feature vectors for the current batch to the list
        all_features.append(batch_features)

    # Concatenate the feature vectors from all batches along the batch dimension
    concatenated_features = torch.cat(all_features, dim=0)
    feature_vectors = concatenated_features.detach().cpu().numpy()
    return feature_vectors

input_array = ndvi_normalized
batch_size = 32
feature_vectors = resnet50_features(input_array, batch_size)
print("Extracted features shape:", feature_vectors.shape)

file_path = "/content/gdrive/MyDrive/Agri_Policy_Proj/ndvi_resnet_fvs.pkl"

# Open a file in binary write mode
with open(file_path, 'wb') as file:
    # Dump the feature vectors into the file using pickle
    pickle.dump(feature_vectors, file)

def train_and_evaluate_rf(features, yields, counties, years, test_size=0.2, components=100):
    # Perform PCA
    pca = PCA(n_components=components)
    reduced_data = pca.fit_transform(features)

    # Split data into train and test sets
    X_train, X_test, y_train, y_test, county_train, county_test, year_train, year_test = train_test_split(
        reduced_data, yields, counties, years, test_size=test_size, random_state=24)

    # Initialize Random Forest regressor
    rf_regressor = RandomForestRegressor()

    # Train the model
    rf_regressor.fit(X_train, y_train)

    # Predict on the test set
    y_pred = rf_regressor.predict(X_test)

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test, y_pred)
    print("Mean Squared Error:", mse)

    # Plot actual vs. predicted values
    plt.figure(figsize=(10, 6))
    plt.plot(y_test, label='Actual', color='blue')
    plt.plot(y_pred, label='Predicted', color='red')
    plt.title(f'Actual vs. Predicted Values for {components} components')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.show()

    predictions_df = pd.DataFrame({'County': county_test, 'Year': year_test, 'Predictions': y_pred})

    # Return trained model
    return rf_regressor, predictions_df

def save_trained_model(trained_model, predictions_df, model_filename, df_filename, parent_directory, ):
    try:
        # Construct the full path to the model file
        model_file_path = f"{parent_directory}/{model_filename}"
        pred_save_path = f"{parent_directory}/{df_filename}"
        # Save the trained model
        joblib.dump(trained_model, model_file_path)

        predictions_df.to_pickle(pred_save_path)
        print(f"Model saved successfully at: {model_file_path}")
    except Exception as e:
        print(f"An error occurred while saving the model: {e}")

trained_model_rf, predictions_df  = train_and_evaluate_rf(feature_vectors, yields, np.array(county), np.array(year) )

model_filename = 'ndvi_rf_model.pkl'
df_filename = 'ndvi_test_pred.pkl'
save_trained_model(trained_model_rf, predictions_df, model_filename, df_filename, parent_directory)

def pca_model_fit(inp_feature_vecs, yields, county, year, model_file_path,components=100):
    # Load trained model
    trained_model = joblib.load(model_file_path)

    # Perform PCA
    pca = PCA(n_components=components)
    reduced_data = pca.fit_transform(inp_feature_vecs)

    # Predict on the test set
    yield_pred = trained_model.predict(reduced_data)

    # Calculate Mean Squared Error
    mse = mean_squared_error(yields, yield_pred)

    predictions_df = pd.DataFrame({'County': county, 'Year': year, 'Predictions': yield_pred})

    return yield_pred, mse, predictions_df

def visualize_predictions(y_pred, target):
    # Plot actual vs. predicted values
    plt.figure(figsize=(10, 6))
    plt.plot(target, label='Actual', color='blue')
    plt.plot(y_pred, label='Predicted', color='red')
    plt.title(f'Actual vs. Predicted Values for 100 components')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.show()

