# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LQjQAlvIPGAz3MBsN18MBMXQYQw3aaQZ
"""

from google.colab import drive
drive.mount('/content/drive/')


import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime, timedelta
from hyperopt import hp, fmin, tpe, Trials, STATUS_OK


import os
import numpy as np
import pandas as pd

file_path = '/content/drive/MyDrive/Conservatives-Official/Water_Quality/water_merged.csv'

df1 = pd.read_csv(file_path)

unique_values_count = df1["county"].value_counts()

sorted_counts = unique_values_count.sort_values(ascending=False)
print(sorted_counts)

df1.columns

filtered_df = df1[df1['county'] == 'Cass']

#filtered_df = filtered_df.drop(['date_site', 'gauge_height','lake_wsl', 'stream_wsl', 'Locationdep', 'Countyname'], axis=1)

print(len(filtered_df))

filtered_df.dropna(subset=['Streamflow'], inplace=True)
print(len(filtered_df))

from sklearn.model_selection import train_test_split

# Sample data loading step
# df = pd.read_csv('your_dataset.csv')

# Assuming 'df' has your time series data in a column named 'Streamflow'
data = filtered_df['Streamflow'].values
data = data.astype('float32').reshape(-1, 1)

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
data_normalized = scaler.fit_transform(data)

# Function to create dataset with look_back
def create_dataset(dataset, look_back=1):
    X, Y = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), 0]
        X.append(a)
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

look_back = 1  # Define how many previous time steps to use to predict the next time step
X, Y = create_dataset(data_normalized, look_back)
X = np.reshape(X, (X.shape[0], 1, X.shape[1]))

# Split the data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

from tensorflow import keras
from tensorflow.keras import layers
from keras_tuner import RandomSearch

def build_model(hp):
    model = keras.Sequential()
    model.add(layers.LSTM(units=hp.Int('units', min_value=4, max_value=64, step=4), input_shape=(1, look_back)))
    model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))
    model.add(layers.Dense(1))
    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
                  loss='mean_squared_error')
    return model

tuner = RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=10,
    executions_per_trial=1,
    directory='tuner_results',
    project_name='streamflow_prediction'
)

tuner.search(X_train, Y_train, epochs=10, validation_data=(X_test, Y_test))

# Get the optimal hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

# Build the model with the best hp.
model = tuner.hypermodel.build(best_hps)

# Train the model
history = model.fit(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test))

# Evaluate the model on the test data
loss = model.evaluate(X_test, Y_test)
print(f'Test loss: {loss}')

# Use the model for predictions
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions)  # Inverting normalization

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

# Print the best hyperparameters
print(f"The best number of units in the LSTM layer: {best_hps.get('units')}")
print(f"The best dropout rate: {best_hps.get('dropout')}")
print(f"The best learning rate for the optimizer: {best_hps.get('learning_rate')}")

model.save('model.h5')

import pickle
from tensorflow.keras.models import load_model

# Assuming 'model' is your Keras model
model.save('temporary_model.h5')  # First, save your model in H5 format

# Then, load it back (this step is optional and just for demonstration)
model = load_model('temporary_model.h5')

# Finally, pickle the model
with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)

def create_dataset(dataset, look_back=1):
    X, Y = [], []
    for i in range(len(dataset)-look_back-1):
        a = dataset[i:(i+look_back), 0]
        X.append(a)
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

# Assuming 'filtered_df' is your DataFrame and it's loaded correctly
filtered_df['date'] = pd.to_datetime(filtered_df['date'])  # Ensure 'date' column is datetime
filtered_df.set_index('date', inplace=True)  # Set 'date' as the index

site_numbers = filtered_df['site_no'].unique()
results = []

for site_no in site_numbers:
    site_data = filtered_df[filtered_df['site_no'] == site_no]

    # Ensure the DataFrame is sorted by date
    site_data = site_data.sort_values('date')

    # Convert the 'Streamflow' column to a numpy array
    data = site_data['Streamflow'].values
    data = data.astype('float32').reshape(-1, 1)

    # Normalize the data
    scaler = MinMaxScaler(feature_range=(0, 1))
    data_normalized = scaler.fit_transform(data)

    # Prepare training data
    look_back = 1
    trainX, trainY = create_dataset(data_normalized, look_back)
    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))

    # Build and compile the LSTM model with optimal parameters
    model = Sequential()
    model.add(LSTM(24, input_shape=(1, look_back)))  # Use 24 units for LSTM layer
    model.add(Dropout(0.1))  # Apply dropout with a rate of 0.4
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.01))  # Use Adam optimizer with a learning rate of 0.01

    # Train the model
    model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=2)

    # Get the last date from the dataset for this site_no
    last_date = site_data.index[-1]

    # Start predictions from the last date in the dataset
    input_seq = data_normalized[-look_back:].reshape((1, 1, look_back))
    prediction_dates = [last_date + timedelta(days=x) for x in range(1, 3*30+1)]  # Next 3 months

    predictions = []
    for _ in range(3*30):  # Predict for each day in the next 3 months
        pred = model.predict(input_seq)
        # Ensure predictions are non-negative
        pred = np.maximum(pred, 0)
        predictions.append(pred[0,0])

        # Update input sequence with the new prediction
        input_seq = np.append(input_seq[0,0,1:], pred).reshape((1, 1, look_back))

    # Invert predictions to original scale
    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))

    # Append predictions with corresponding site_no and date to results
    site_predictions = [(site_no, date.strftime('%Y-%m-%d'), prediction[0]) for date, prediction in zip(prediction_dates, predictions)]
    results.extend(site_predictions)

# Convert results to DataFrame for easier handling
results_df = pd.DataFrame(results, columns=['site_no', 'date', 'prediction'])

# Ensure 'date' column is in the correct datetime format
results_df['date'] = pd.to_datetime(results_df['date'])



print(results_df.head(20))

min_prediction = results_df['prediction'].min()
max_prediction = results_df['prediction'].max()

print(f"Minimum prediction value: {min_prediction}")
print(f"Maximum prediction value: {max_prediction}")

results_df.to_csv('/content/drive/MyDrive/Conservatives-Official/Water_Quality/Lstm_water_pred.csv', index=False)