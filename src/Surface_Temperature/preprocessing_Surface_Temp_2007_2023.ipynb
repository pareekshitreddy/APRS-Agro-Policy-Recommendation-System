{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkRjjFowO4pv"
      },
      "outputs": [],
      "source": [
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import mapping\n",
        "from rasterio.crs import CRS\n",
        "\n",
        "def generateNumpyFromTif(fileName, geoJson):\n",
        "\n",
        "  # Load the GeoTIFF file of the entire state\n",
        "  with rasterio.open(fileName) as state_src:\n",
        "      state_data = state_src.read(1)  # Read raster data\n",
        "      state_meta = state_src.meta    # Get metadata\n",
        "      print(state_meta)\n",
        "\n",
        "      # Define the CRS of the state raster\n",
        "      state_crs = CRS.from_epsg(5070)  # Assuming EPSG:5070\n",
        "      state_crs = state_meta['crs']\n",
        "\n",
        "      # Load the shapefile containing county boundaries\n",
        "      counties = gpd.read_file(geoJson)\n",
        "      # Set CRS for county boundaries\n",
        "      counties = counties.set_crs(epsg=4326, allow_override=True)\n",
        "      # Reproject county boundaries to match the CRS of the state raster\n",
        "      counties = counties.to_crs(state_crs)\n",
        "\n",
        "      # Create an empty list to store masked datasets\n",
        "      masked_datasets = []\n",
        "      # Iterate over each county\n",
        "      for i, county in counties.iterrows():\n",
        "          # Convert the county boundary to GeoJSON format\n",
        "          county_geojson = mapping(county['geometry'])\n",
        "          try: # Mask the state data by the county boundary\n",
        "            masked_data, masked_transform = mask(state_src, [county_geojson], crop=True)\n",
        "            # Append the masked dataset to the list\n",
        "            masked_datasets.append((masked_data[0], county['countyfp']))\n",
        "          except:\n",
        "            pass\n",
        "  return masked_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL2g7Rd2XDjR"
      },
      "outputs": [],
      "source": [
        "arr = generateNumpyFromTif('/content/drive/MyDrive/Conservatives-Official/Surface_Temperature/Data/MODIS_Surface_Temperature/avg_surface_temp_Mar_to_May_2015.tif', '/content/drive/MyDrive/Conservatives-Official/Surface_Temperature/Data/South_Dakota_County_Boundaries.geojson')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIQiRVa6Yn9j"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "# Create a new HDF5 file\n",
        "with h5py.File('data.h5', 'w') as f:\n",
        "    # Create datasets within the file\n",
        "    f.create_dataset('dataset_name', data=data_array)\n",
        "    # You can create multiple datasets as needed\n",
        "    # f.create_dataset('another_dataset', data=another_data_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xR8HAks270e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_1 = pd.read_csv('/content/drive/MyDrive/Conservatives-Official/Surface_Temperature/Data/F59D4B49-DA13-3186-B13D-09EB8A55030B.csv')\n",
        "df_1 = pd.DataFrame(df_1)\n",
        "#un = df[\"Data Item\"].unique()\n",
        "df_1 = df_1[df_1[\"Data Item\"] == 'WHEAT, SPRING, (EXCL DURUM) - YIELD, MEASURED IN BU / ACRE']\n",
        "df_1.columns\n",
        "filtered_data_1 = df_1[['State ANSI','County ANSI','Year', 'Value']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4FN9yeW256q"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Conservatives-Official/Surface_Temperature/Data/CC467883-C048-3E63-8CEC-477F196EAEE2.csv')\n",
        "df = pd.DataFrame(df)\n",
        "#un = df[\"Data Item\"].unique()\n",
        "df = df[df[\"Data Item\"] == 'WHEAT, SPRING, (EXCL DURUM) - YIELD, MEASURED IN BU / ACRE']\n",
        "df.columns\n",
        "filtered_data = df[['State ANSI','County ANSI','Year', 'Value']]\n",
        "filtered_data.fillna(0, inplace=True)\n",
        "filtered_data.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "filtered_data['County ANSI'] = filtered_data['County ANSI'].astype(int)\n",
        "filtered_data['Year'] = filtered_data['Year'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLY119y223CL"
      },
      "outputs": [],
      "source": [
        "df_concatenated = pd.concat([filtered_data, filtered_data_1], axis=0, ignore_index=True)\n",
        "df_concatenated.head(10)\n",
        "df_concatenated.fillna(0, inplace=True)\n",
        "df_concatenated.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_concatenated['County ANSI'] =df_concatenated['County ANSI'].astype(int)\n",
        "df_concatenated['Year'] = df_concatenated['Year'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAxVcbiRvn9e"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Define the new dimensions after resizing\n",
        "new_height = 100\n",
        "new_width = 100\n",
        "\n",
        "\n",
        "\n",
        "states = ['46']\n",
        "years = ['2007','2008','2009','2010','2011','2012','2013','2014','2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
        "\n",
        "# Create or open the HDF5 file\n",
        "with h5py.File('/content/drive/MyDrive/Conservatives-Official/Surface_Temperature/Pickle_files/south_dakota_wheat.h5', 'a') as hf:\n",
        "    # Check if the dataset exists; if not, create it\n",
        "    if 'features' not in hf:\n",
        "        hf.create_dataset('features', shape=(0, new_height, new_width), maxshape=(None, new_height, new_width), chunks=True)\n",
        "        hf.create_dataset('county', shape=(0,1), maxshape=(None, 1), chunks=True)\n",
        "        hf.create_dataset('year', shape=(0,1), maxshape=(None, 1), chunks=True)\n",
        "        hf.create_dataset('state', shape=(0,1), maxshape=(None, 1), chunks=True)\n",
        "        hf.create_dataset('yield', shape=(0,1), maxshape=(None, 1), chunks=True)\n",
        "\n",
        "    # Loop over years and states\n",
        "    for year in years:\n",
        "        for state in states:\n",
        "            new_X = generateNumpyFromTif(fileName=f'/content/drive/MyDrive/MODIS_Surface_Temperature/avg_surface_temp_Mar_to_May_{year}.tif', geoJson='/content/drive/MyDrive/Conservatives-Official/Surface_Temperature/Data/MODIS_Surface_Temperature/avg_surface_temp_Mar_to_May_2015.tif', '/content/drive/MyDrive/Conservatives-Official/Surface_Temperature/Data/South_Dakota_County_Boundaries.geojson')\n",
        "            for i in range(len(new_X)):\n",
        "              data = new_X[i][0]\n",
        "              # Resize the data\n",
        "              resized_data = resize(data, (new_height, new_width), anti_aliasing=True)\n",
        "              county = new_X[i][1]\n",
        "              # Append new data to the HDF5 file\n",
        "              # print(hf['features'].shape)\n",
        "              temp = df_concatenated[df_concatenated['Year']==int(year)]\n",
        "              temp = temp[temp['State ANSI']== 46]\n",
        "              temp = temp[temp['County ANSI'] == int(county)]# get from the  excel file with respect to the county FSID and the year of SD\n",
        "              yld = temp['Value']\n",
        "              if not yld.empty:\n",
        "                  yld = yld.iloc[0]  # Using iloc method\n",
        "                  hf['features'].resize(hf['features'].shape[0] + 1, axis=0)  # Increase dataset size by 1\n",
        "                  # print(hf['features'].shape, hf['features'][-1].shape, resized_data.shape)\n",
        "                  hf['features'][-1] = resized_data\n",
        "                  hf['county'].resize(hf['county'].shape[0]+1, axis=0)\n",
        "                  hf['county'][-1] = county\n",
        "                  hf['year'].resize(hf['year'].shape[0]+1, axis=0)\n",
        "                  hf['year'][-1] = year\n",
        "                  hf['state'].resize(hf['state'].shape[0]+1, axis=0)\n",
        "                  hf['state'][-1] = state\n",
        "                  hf['yield'].resize(hf['yield'].shape[0]+1, axis=0)\n",
        "                  hf['yield'][-1] = yld\n",
        "              else: pass\n",
        "\n",
        "    print(hf['features'].shape, hf['county'].shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
